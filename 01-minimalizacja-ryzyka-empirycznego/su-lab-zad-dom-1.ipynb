{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Systemy uczące się - Zad. dom. 1: Minimalizacja ryzyka empirycznego\n",
    "Celem zadania jest zaimplementowanie własnego drzewa decyzyjnego wykorzystującego idee minimalizacji ryzyka empirycznego. \n",
    "\n",
    "### Autor rozwiązania\n",
    "Uzupełnij poniższe informacje umieszczając swoje imię i nazwisko oraz numer indeksu:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "NAME = \"Bartłomiej Andree\"\n",
    "ID = \"162961\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-12T17:49:02.923443Z",
     "start_time": "2025-03-12T17:49:02.913194Z"
    }
   },
   "outputs": [],
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twoja implementacja\n",
    "\n",
    "Twoim celem jest uzupełnić poniższą klasę `TreeNode` tak by po wywołaniu `TreeNode.fit` tworzone było drzewo decyzyjne minimalizujące ryzyko empiryczne. Drzewo powinno wspierać problem klasyfikacji wieloklasowej (jak w przykładzie poniżej). Zaimplementowany algorytm nie musi (ale może) być analogiczny do zaprezentowanego na zajęciach algorytmu dla klasyfikacji. Wszelkie przejawy inwencji twórczej wskazane. **Pozostaw komenatrze w kodzie, które wyjaśniają Twoje rozwiązanie.**\n",
    "\n",
    "Schemat oceniania:\n",
    "- wynik na zbiorze Iris (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 10%: +40%,\n",
    "- wynik na ukrytym zbiorze testowym 1 (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 15%: +30%,\n",
    "- wynik na ukrytym zbiorze testowym 2 (automatyczna ewaluacja) celność klasyfikacji >= prostego baseline'u + 5%: +30%.\n",
    "\n",
    "Niedozwolone jest korzystanie z zewnętrznych bibliotek do tworzenia drzewa decyzyjnego (np. scikit-learn). \n",
    "Możesz jedynie korzystać z biblioteki numpy.\n",
    "\n",
    "#### Uwaga: Możesz dowolnie modyfikować elementy tego notebooka (wstawiać komórki i zmieniać kod), o ile będzie się w nim na koniec znajdowała kompletna implementacja klasy `TreeNode` w jednej komórce."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:49:02.998944Z",
     "start_time": "2025-03-12T17:49:02.977394Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self):\n",
    "        self.left: TreeNode | None = None   # wierzchołek znajdujący się po lewej stronie\n",
    "        self.right: TreeNode | None = None  # wierzchołek znajdujący się po prawej stronie\n",
    "        self.value = None                   # wartość liścia (przypisana odpowiedź)\n",
    "        self.split_feature = None           # indeks cechy, po której dzielimy\n",
    "        self.split_threshold = None         # próg podziału\n",
    "\n",
    "    def _gini(self, y):\n",
    "        \"\"\"\n",
    "        Funkcja obliczająca współczynnik Gini dla zbioru etykiet y.\n",
    "        Współczynnik Gini mierzy stopień zanieczyszczenia zbioru - im mniejsza wartość,\n",
    "        tym czystszy zbiór (więcej przykładów z tej samej klasy).\n",
    "        \"\"\"\n",
    "        # Dla pustego zbioru zwracamy 0, bo nie ma w nim zanieczyszczenia\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "\n",
    "        # Zliczamy ile mamy przykładów z każdej klasy\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "        # Obliczamy prawdopodobieństwo wystąpienia każdej z klas\n",
    "        # To po prostu liczba przykładów danej klasy podzielona przez liczbę wszystkich przykładów\n",
    "        probabilities = counts / len(y)\n",
    "\n",
    "        # Wzór na współczynnik Gini: 1 - suma(p_i^2)\n",
    "        # 1 minus suma kwadratów prawdopodobieństw dla każdej klasy\n",
    "        # Im mniejszy jest współczynnik Gini, tym lepiej - bardziej jednorodny zbiór\n",
    "        gini = 1 - np.sum(probabilities ** 2)\n",
    "\n",
    "        return gini\n",
    "\n",
    "    def _find_split(self, data, target):\n",
    "        \"\"\"\n",
    "        Szuka najlepszego podziału danych według kryterium Giniego.\n",
    "        Przeszukuje wszystkie cechy i wszystkie możliwe wartości progów podziału,\n",
    "        wybierając ten podział, który daje najmniejszy ważony współczynnik Gini.\n",
    "        To oznacza, że dzieli dane na najbardziej homogeniczne podzbiory.\n",
    "        \"\"\"\n",
    "        # Zaczynamy od najgorszego możliwego podziału (nieskończony Gini)\n",
    "        best_gini = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        n_samples, n_features = data.shape\n",
    "\n",
    "        # Sprawdzamy każdą cechę jako potencjalny kandydat do podziału\n",
    "        for feature in range(n_features):\n",
    "            # Bierzemy wszystkie unikalne wartości tej cechy jako potencjalne progi podziału\n",
    "            # Nie ma sensu sprawdzać wartości, które się powtarzają, bo dadzą ten sam podział\n",
    "            unique_values = np.unique(data[:, feature])\n",
    "\n",
    "            # Testujemy każdą unikalną wartość jako próg\n",
    "            for threshold in unique_values:\n",
    "                # Tworzymy maski boolowskie dla lewej i prawej strony podziału\n",
    "                # Lewa strona to wszystkie przykłady, gdzie wartość cechy ≤ progu\n",
    "                left_mask = data[:, feature] <= threshold\n",
    "                # Prawa strona to wszystkie pozostałe przykłady\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                # Jeśli któraś strona jest pusta, to podział jest bezużyteczny\n",
    "                # (nie dzieli zbioru na dwie części). Pomijamy taki podział.\n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Obliczamy współczynnik Gini dla obu stron podziału\n",
    "                gini_left = self._gini(target[left_mask])\n",
    "                gini_right = self._gini(target[right_mask])\n",
    "\n",
    "                # Obliczamy średni ważony współczynnik Gini dla całego podziału\n",
    "                # Waga każdej strony zależy od liczby przykładów po tej stronie\n",
    "                n_left = np.sum(left_mask)\n",
    "                n_right = np.sum(right_mask)\n",
    "                gini_split = (n_left/n_samples) * gini_left + (n_right/n_samples) * gini_right\n",
    "\n",
    "                # Jeśli ten podział daje lepszy (mniejszy) współczynnik Gini niż dotychczasowy najlepszy,\n",
    "                # to zapamiętujemy go jako nowy najlepszy podział\n",
    "                # wynika to z faktu, że minimalizujemy ryzyko empiryczne\n",
    "                if gini_split < best_gini:\n",
    "                    best_gini = gini_split\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        # Zwracamy indeks najlepszej cechy, najlepszy próg i wartość Gini dla tego podziału\n",
    "        return best_feature, best_threshold, best_gini\n",
    "\n",
    "    def fit(self, data: np.ndarray, target: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Trenuje drzewo decyzyjne używając algorytmu minimalizacji ryzyka empirycznego.\n",
    "        Rekurencyjnie dzieli dane na podstawie kryterium Giniego, aby stworzyć drzewo\n",
    "        klasyfikujące przykłady do odpowiednich klas.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "            target (np.ndarray): wektor klas o długości n, gdzie n to liczba przykładów\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Poniej znajdziesz przykładowy \"pseudo-kod\" rozwiązania, nie musisz się go trzymać\n",
    "\t\t# (możesz zaimplementować to w inny sposób, jeżeli wolisz)\n",
    "\t\t#\n",
    "\t\t# Znajdź najlepszy podział x, y\n",
    "\t\t# if uzyskano poprawę funkcji celu (bądź inny, zaproponowany przez Ciebie warunek):\n",
    "\t\t# \tpodziel dane na dwie części data_left i data_right, zgodnie z warunkiem\n",
    "\t\t# \tself.left = Node()\n",
    "\t\t# \tself.right = Node()\n",
    "\t\t# \tself.left.fit(data_left)\n",
    "\t\t# \tself.right.fit(data_right)\n",
    "\t\t# else:\n",
    "\t\t# \tobecny Node jest liściem, zapisz jego odpowiedź\n",
    "\n",
    "\n",
    "        # Jeśli wszystkie przykłady należą do tej samej klasy, tworzymy liść z tą klasą jako wartością\n",
    "        if len(np.unique(target)) == 1:\n",
    "            # Wartość liścia to klasa wszystkich przykładów w tym węźle\n",
    "            self.value = target[0]\n",
    "            return\n",
    "\n",
    "        # Szukamy najlepszego podziału minimalizującego współczynnik Gini - czyli minimalizującego ryzyko empiryczne\n",
    "        best_feature, best_threshold, _ = self._find_split(data, target)\n",
    "\n",
    "        # Jeśli nie udało się znaleźć sensownego podziału, tworzymy liść z najczęstszą klasą jako wartością\n",
    "        if best_feature is None:\n",
    "            # Używamy np.bincount do znalezienia najczęstszej klasy\n",
    "            # argmax zwraca indeks największej wartości w tablicy\n",
    "            self.value = np.bincount(target).argmax()\n",
    "            return\n",
    "\n",
    "        # Jeśli znaleźliśmy dobry podział, zapisujemy jego parametry\n",
    "        self.split_feature = best_feature\n",
    "        self.split_threshold = best_threshold\n",
    "\n",
    "        # Dzielimy dane na dwa podzbiory według znalezionego podziału\n",
    "        left_mask = data[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "\n",
    "        # Dodatkowe sprawdzenie, czy podział faktycznie dzieli dane\n",
    "        # (raczej niepotrzebne, bo sprawdzamy to wcześniej, ale dla pewności)\n",
    "        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "            # Jeśli nie dzieli, tworzymy liść zamiast podwęzłów\n",
    "            self.value = np.bincount(target).argmax()\n",
    "            return\n",
    "\n",
    "        # Tworzymy lewy i prawy węzeł dziecka\n",
    "        self.left = TreeNode()\n",
    "        self.right = TreeNode()\n",
    "\n",
    "        # Rekurencyjnie trenujemy lewe i prawe poddrzewo na odpowiednich podzbiorach danych\n",
    "        # Tu dzieje się główna idea budowy drzewa - dziel i zwyciężaj\n",
    "        self.left.fit(data[left_mask], target[left_mask])\n",
    "        self.right.fit(data[right_mask], target[right_mask])\n",
    "\n",
    "    def predict(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Przewiduje klasy dla nowych danych na podstawie wytrenowanego drzewa.\n",
    "        Przechodzi drzewo dla każdego przykładu i zwraca klasę z odpowiedniego liścia.\n",
    "\n",
    "        Args:\n",
    "            data (np.ndarray): macierz cech o wymiarach (n, m), gdzie n to liczba przykładów, a m to liczba cech\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: wektor przewidzianych klas o długości n, gdzie n to liczba przykładów\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Poniżej znajdziesz przykładowy \"pseudo-kod\" rozwiązania, nie musisz się go trzymać\n",
    "\t\t# (możesz zaimplementować to w inny sposób, jeżeli wolisz),\n",
    "\t\t# ważne by metoda TreeNode.predict zwracała wektor przewidzianych klas\n",
    "\t\t#\n",
    "        # Dla każdego przykładu w data:\n",
    "\t\t#   node = self\n",
    "        #   if node nie jest liściem:\n",
    "        #       if warunek podziału node jest spełniony:\n",
    "        #           node = node.right\n",
    "        #       else:\n",
    "        #           node = node.left\n",
    "        #   y_pred[i] = zwróć wartość node (liść)\n",
    "\n",
    "\n",
    "        # Inicjujemy wektor predykcji zerami\n",
    "        y_pred = np.zeros(data.shape[0], dtype=int)\n",
    "\n",
    "        # Dla każdego przykładu z danych wejściowych...\n",
    "        for i, sample in enumerate(data):\n",
    "            # Zaczynamy od korzenia drzewa\n",
    "            node = self\n",
    "\n",
    "            # Przechodzimy w dół drzewa, dopóki nie dotrzemy do liścia\n",
    "            # Liść to węzeł, który ma przypisaną konkretną wartość klasy\n",
    "            while node.value is None:\n",
    "                # Sprawdzamy warunek podziału dla bieżącego węzła\n",
    "                if sample[node.split_feature] <= node.split_threshold:\n",
    "                    # Jeśli wartość cechy jest mniejsza lub równa progowi, idziemy w lewo\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    # W przeciwnym razie idziemy w prawo\n",
    "                    node = node.right\n",
    "\n",
    "            # Gdy dotrzemy do liścia, przypisujemy jego wartość jako predykcję dla tego przykładu\n",
    "            y_pred[i] = node.value\n",
    "\n",
    "        return y_pred"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykład trenowanie i testowania drzewa\n",
    " \n",
    "Później znajduje się przykład trenowania i testowania drzewa na zbiorze danych `iris`, który zawierający 150 próbek irysów, z czego każda próbka zawiera 4 atrybuty: długość i szerokość płatków oraz długość i szerokość działki kielicha. Każda próbka należy do jednej z trzech klas: `setosa`, `versicolor` lub `virginica`, które są zakodowane jak int.\n",
    "\n",
    "Możesz go wykorzystać do testowania swojej implementacji. Możesz też zaimplementować własne testy lub użyć innych zbiorów danych, np. innych [zbiorów danych z scikit-learn](https://scikit-learn.org/stable/datasets/toy_dataset.html#toy-datasets)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:49:03.098902Z",
     "start_time": "2025-03-12T17:49:03.089935Z"
    }
   },
   "cell_type": "code",
   "source": "#!pip install scikit-learn",
   "outputs": [],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T17:49:03.205987Z",
     "start_time": "2025-03-12T17:49:03.169763Z"
    }
   },
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33, random_state=2024)\n",
    "\n",
    "tree_model = TreeNode()\n",
    "tree_model.fit(X_train, y_train)\n",
    "y_pred = tree_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84\n"
     ]
    }
   ],
   "execution_count": 59
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Systemy uczące się - Zad. dom. 11: Algorytm oczekiwanie-maksymalizacja\n",
    "\n",
    "### Autor rozwiązania\n",
    "Uzupełnij poniższe informacje umieszczając swoje imię i nazwisko oraz numer indeksu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "NAME = \"Bartłomiej Andree\"\n",
    "ID = \"162961\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ćwiczenie 1 - implementacja k-means\n",
    "Wygeneruj zbiór danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.datasets.samples_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mmatplotlib\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minline\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msamples_generator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_blobs\n\u001b[32m      5\u001b[39m X, _ = make_blobs(n_samples=\u001b[32m300\u001b[39m, centers=\u001b[32m4\u001b[39m,\n\u001b[32m      6\u001b[39m                   cluster_std=\u001b[32m0.60\u001b[39m, random_state=\u001b[32m0\u001b[39m)\n\u001b[32m      8\u001b[39m plt.scatter(X[:, \u001b[32m0\u001b[39m], X[:, \u001b[32m1\u001b[39m], s=\u001b[32m50\u001b[39m);\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn.datasets.samples_generator'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=300, centers=4,\n",
    "                  cluster_std=0.60, random_state=0)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorytm k-średnich, zaraz po inicjalizacji początkowych wartości centroidów, składa się z 2 podstawowych, powtarzanych iteracyjnie kroków:\n",
    "- *expectation* - przypisanie każdego elementu zbiory danych do najbliższego centroidu\n",
    "- *maximization* - aktualizacja centroidów. Centroid jest wyznaczany jako średnia arytmetyczna po wszystkich przypisanych do niego punktach\n",
    "\n",
    "Zaimplementuj ten algorytm. W implementacji może być pomocna funkcja `pairwise_distances_argmin` [[dokumentacja]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances_argmin.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e61595a34394b33ba14152590ba2f96",
     "grade": true,
     "grade_id": "cell-e0380f9119bde196",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_clusters(x: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"Znajduje skupienia algorytmem k-średnich\n",
    "    \n",
    "    Na wyjście zwracany jest wektor zawierający przypisania każdego elementu z x do skupienia\n",
    "    np. [0,1,1] dla 3-elementowego X i k=2 oznacza że pierwszy element należy\n",
    "    do grupy pierwszej, a element drugi i trzeci należą do grupy drugiej.\n",
    "    \n",
    "    Arguments:\n",
    "        x (np.ndarray): zbiór danych\n",
    "        k (int): liczba szukanych grup\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: wektor zawierający przypisania każdego elementu z x do skupienia   \n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Zaimplementuj funkcję\n",
    "    from sklearn.metrics import pairwise_distances_argmin\n",
    "    \n",
    "    # Liczba próbek, wymiar danych\n",
    "    n_samples, n_features = x.shape\n",
    "    \n",
    "    # Losowo wybieramy k punktów z danych\n",
    "    indices = np.random.choice(n_samples, k, replace=False)\n",
    "    centroids = x[indices]\n",
    "    \n",
    "    # Iterujemy aż do zbieżności\n",
    "    labels_old = np.zeros(n_samples)\n",
    "    labels = np.zeros(n_samples)\n",
    "    max_iterations = 100\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        # Przypisanie punktów do najbliższych centroidów\n",
    "        labels = pairwise_distances_argmin(x, centroids)\n",
    "        \n",
    "        # Sprawdzenie zbieżności\n",
    "        if np.all(labels == labels_old):\n",
    "            break\n",
    "        \n",
    "        labels_old = labels.copy()\n",
    "        \n",
    "        # Aktualizacja centroidów\n",
    "        for j in range(k):\n",
    "            # Jeśli brak punktów w klastrze, zachowujemy poprzedni centroid\n",
    "            if np.sum(labels == j) == 0:\n",
    "                continue\n",
    "            # Nowy centroid jako średnią punktów w klastrze\n",
    "            centroids[j] = np.mean(x[labels == j], axis=0)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = find_clusters(X, 4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przetestuj działanie metody dla innych danych:\n",
    "```\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "X, _ = make_moons(200, noise=.05, random_state=0)\n",
    "```\n",
    "oraz\n",
    "```\n",
    "X,_ = make_circles(n_samples=1000, factor=0.3, noise=0.1)\n",
    "```\n",
    "czy algorytm k-średnich potrafi je zgrupować w sposób zgodny z oczekiwaniami? Jakie rodzaje kształtów potrafi wykrywać algorytm k-średnich?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Algorytm k-means nie radzi sobie dobrze z grupowaniem danych o niekonwencjonalnych kształtach, takich jak półksiężyce czy okręgi. Jest to spowodowane fundamentalnym założeniem algorytmu, że skupiska są w przybliżeniu sferyczne/kuliste i równomiernie rozłożone w przestrzeni.\n",
    "\n",
    "W przypadku zbiorów danych half-moons, algorytm dzieli dane na dwie grupy, ale nie według naturalnych kształtów półksiężyców, tylko wzdłuż linii prostej, która minimalizuje odległości euklidesowe do centroidów. W rezultacie obie grupy zawierają punkty z obu półksiężyców, co nie odpowiada intuicyjnemu podziałowi.\n",
    "\n",
    "W przypadku zbiorów danych circles, algorytm dzieli dane na zewnętrzny i wewnętrzny okrąg, ale często nierównomiernie, dzieląc zewnętrzny okrąg na dwie części, lub łącząc części wewnętrznego i zewnętrznego okręgu. Nie jest w stanie wykryć koncentrycznych okręgów jako naturalnych skupisk.\n",
    "\n",
    "Algorytm k-means dobrze wykrywa skupiska, które są:\n",
    "- Sferyczne (kuliste)\n",
    "- O podobnej wariancji\n",
    "- O podobnej liczebności\n",
    "- Dobrze odseparowane od siebie\n",
    "\n",
    "Nie radzi sobie z:\n",
    "- Skupiskami o złożonych kształtach\n",
    "- Skupiskami o różnej gęstości\n",
    "- Skupiskami o bardzo różnych wielkościach\n",
    "- Skupiskami, które nie są liniowo separowalne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przetesuj działanie profesjonalnej implementacji algorytm k-średnich z pakietu sklearn\n",
    "```\n",
    "from sklearn.cluster import KMeans\n",
    "```\n",
    "ponieważ problem jest nienadzorowany, funkcja `fit()` przyjmie tylko jeden argument (`X`). Liczbę grup możesz określić poprzez parametr konstruktora `n_clusters = 5`. Przypisanie obserwacji do poszczególnych grup możesz odczytać z włąściwości `labels_` wytrenowanego obiektu `KMeans`. Narysuj wynik grupowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b727a6112c2c10ca94dd7a81ec227ef1",
     "grade": false,
     "grade_id": "cell-e1b3931a95eb2847",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: wykonaj opisane testy\n",
    "# half-moons\n",
    "from sklearn.datasets import make_moons\n",
    "X_moons, _ = make_moons(200, noise=.05, random_state=0)\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Oryginalne dane (half-moons)\")\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], s=50)\n",
    "\n",
    "# Grupowanie k-means (k=2)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Grupowanie k-means (k=2)\")\n",
    "labels_moons = find_clusters(X_moons, 2)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons, s=50, cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# circles\n",
    "from sklearn.datasets import make_circles\n",
    "X_circles, _ = make_circles(n_samples=1000, factor=0.3, noise=0.1)\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Oryginalne dane (circles)\")\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], s=50)\n",
    "\n",
    "# Grupowanie k-means (k=2)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Grupowanie k-means (k=2)\")\n",
    "labels_circles = find_clusters(X_circles, 2)\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=labels_circles, s=50, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O algorytmie k-średnich często mówi się, że wykrywa skupiska sferyczne.\n",
    "```\n",
    "from helpers import get_quasispherical_data\n",
    "X = get_quasispherical_data()```\n",
    "\n",
    "Sprawdź tę hipotezę na podanych trudniejszych danych sferycznych (no właśnie czy są one sferyczne?). Czy nawet gdyby wybrać centroidy jako prawdziwe centra skupisk, otrzymalibyśmy prawidłowe grupowanie? Dlaczego?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a631c6c5e47e6d5a14fa01a1a36cd8ae",
     "grade": true,
     "grade_id": "cell-33fb7f006da96a45",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: wykonaj opisane testy\n",
    "# Testujemy implementację z sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Testowanie na oryginalnych danych\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(X)\n",
    "labels_sklearn = kmeans.labels_\n",
    "\n",
    "# Porównanie wyników naszej implementacji i sklearn\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Nasza implementacja (k=4)\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Implementacja sklearn (k=4)\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels_sklearn, s=50, cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# Testowanie na half-moons\n",
    "kmeans_moons = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans_moons.fit(X_moons)\n",
    "labels_moons_sklearn = kmeans_moons.labels_\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Half-Moons - Nasza implementacja (k=2)\")\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons, s=50, cmap='viridis')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Half-Moons - Implementacja sklearn (k=2)\")\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_moons_sklearn, s=50, cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# Testowanie na circles\n",
    "kmeans_circles = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans_circles.fit(X_circles)\n",
    "labels_circles_sklearn = kmeans_circles.labels_\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Circles - Nasza implementacja (k=2)\")\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=labels_circles, s=50, cmap='viridis')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Circles - Implementacja sklearn (k=2)\")\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=labels_circles_sklearn, s=50, cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# Wczytanie danych quasispherical\n",
    "from helpers import get_quasispherical_data\n",
    "X_quasi = get_quasispherical_data()\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Dane quasispherical\")\n",
    "plt.scatter(X_quasi[:, 0], X_quasi[:, 1], s=50)\n",
    "plt.show()\n",
    "\n",
    "# Grupowanie k-means z własnej implementacji\n",
    "labels_quasi = find_clusters(X_quasi, 3)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Nasza implementacja (k=3)\")\n",
    "plt.scatter(X_quasi[:, 0], X_quasi[:, 1], c=labels_quasi, s=50, cmap='viridis')\n",
    "\n",
    "# Porównanie z sklearn\n",
    "kmeans_quasi = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans_quasi.fit(X_quasi)\n",
    "labels_quasi_sklearn = kmeans_quasi.labels_\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Implementacja sklearn (k=3)\")\n",
    "plt.scatter(X_quasi[:, 0], X_quasi[:, 1], c=labels_quasi_sklearn, s=50, cmap='viridis')\n",
    "plt.show()\n",
    "\n",
    "# Test z centroidami w prawdziwych centrach skupisk\n",
    "# Dla demonstracji użyjemy sztucznie ustawionych centroidów w środkach klastrów\n",
    "# Załóżmy, że centra są mniej więcej w punktach: (-2, 0), (2, 0), (0, 2)\n",
    "centers = np.array([[-2, 0], [2, 0], [0, 2]])\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "# Przypisujemy punkty do najbliższych centroidów\n",
    "labels_fixed_centers = pairwise_distances_argmin(X_quasi, centers)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Grupowanie z ustalonymi centroidami w centrach skupisk\")\n",
    "plt.scatter(X_quasi[:, 0], X_quasi[:, 1], c=labels_fixed_centers, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5, marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Na podstawie testów można zauważyć, że dane quasispherical nie są idealnie sferyczne, lecz mają kształt eliptyczny lub wydłużony. Nawet przy idealnym wyborze centroidów w rzeczywistych środkach klastrów (oznaczonych czerwonymi gwiazdkami na ostatnim wykresie), algorytm k-means nie zawsze daje prawidłowe grupowanie.\n",
    "\n",
    "Dzieje się tak, ponieważ k-means dzieli przestrzeń na obszary na podstawie odległości euklidesowej od centroidów, tworząc tzw. diagramy Voronoia. Jeśli skupiska są wydłużone, eliptyczne lub o nierównomiernej gęstości, to granica podziału wyznaczona przez algorytm k-means może przecinać rzeczywiste skupienie, przypisując część punktów należących do jednego naturalnego klastra do innego klastra.\n",
    "\n",
    "W przypadku danych quasi-sferycznych widzimy, że niektóre punkty z jednego naturalnego klastra mogą być przypisane do innego klastra ze względu na ich odległość od centroidu, nawet jeśli centroidy są idealnie umieszczone. Jest to fundamentalne ograniczenie algorytmu k-means, który zakłada, że:\n",
    "1. Skupiska mają zbliżony kształt i wielkość\n",
    "2. Odległość euklidesowa jest odpowiednią miarą podobieństwa\n",
    "3. Centroidy dobrze reprezentują swoje skupienia\n",
    "\n",
    "Dlatego nawet przy idealnym wyborze centroidów możemy nie uzyskać prawidłowego grupowania dla danych, które odbiegają od założeń algorytmu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "- Uruchom swój algorytm kilka razy - czy za każdym razem dostajesz ten sam wynik? Co ci to mówi o specyfice rozwiązowanego problemu i algortmie k-średnich jako algorytmie optymalizacyjnym?\n",
    "- Twój algorytm uruchamiałeś z apriori znaną liczbą grup $k=4$, przetestuj działanie algorytmu dla $k=5$. Uruchom swój algorytm kilka razy - czy za każdym razem dostajesz ten sam wynik? Co ci to mówi o algortmie k-średnich jako algorytmie optymalizacyjnym?\n",
    "- Jak w praktyce możemy próbować choć trochę ograniczyć problem algorytmu dot. utykania w minimach lokalnych?\n",
    "\n",
    "**Odpowiedzi na pytania:**\n",
    "\n",
    "1. **Czy za każdym razem dostajesz ten sam wynik?**\n",
    "   Nie, za każdym uruchomieniem algorytmu k-means możemy otrzymać różne wyniki. Jest to spowodowane losową inicjalizacją centroidów. W zależności od wyboru początkowych centroidów, algorytm może konwergować do różnych minimów lokalnych funkcji kosztu.\n",
    "\n",
    "2. **Co to mówi o specyfice rozwiązywania problemu i algorytmie k-średnich jako algorytmie optymalizacyjnym?**\n",
    "   Algorytm k-means minimalizuje sumę kwadratów odległości punktów od ich centroidów. Jest to problem optymalizacyjny, który ma wiele minimów lokalnych. Algorytm k-means jest algorytmem zachłannym, który zawsze konwerguje do minimum lokalnego, ale niekoniecznie do globalnego. Oznacza to, że wynik grupowania może zależeć od początkowych warunków (inicjalizacji centroidów) i algorytm może \"utknąć\" w różnych minimach lokalnych, dając różne grupowania dla tych samych danych.\n",
    "\n",
    "3. **Czy dla k=5 za każdym razem dostajesz ten sam wynik?**\n",
    "   Dla k=5 (większego niż prawdziwa liczba klastrów w danych, która wynosi 4), różnice w wynikach poszczególnych uruchomień są jeszcze bardziej widoczne. Algorytm próbuje znaleźć 5 klastrów, mimo że naturalnie w danych występują 4 klastry. To powoduje, że jeden z naturalnych klastrów zostanie podzielony, a dokładny sposób podziału będzie zależał od inicjalizacji centroidów.\n",
    "\n",
    "4. **Co to mówi o algorytmie k-średnich jako algorytmie optymalizacyjnym?**\n",
    "   Pokazuje to, że algorytm k-means jest wrażliwy zarówno na inicjalizację, jak i na wybór parametru k. Gdy k jest większe niż rzeczywista liczba klastrów, problem ma jeszcze więcej możliwych minimów lokalnych, co zwiększa zmienność wyników. Algorytm zawsze dąży do minimalizacji funkcji kosztu, ale ta funkcja może mieć wiele minimów lokalnych o podobnych wartościach, co prowadzi do różnych grupowań.\n",
    "\n",
    "5. **Jak w praktyce można ograniczyć problem utykania w minimach lokalnych?**\n",
    "   W praktyce można stosować kilka strategii:\n",
    "   - **Wielokrotne uruchomienie**: Uruchomienie algorytmu wiele razy z różnymi inicjalizacjami i wybranie najlepszego wyniku (o najniższej wartości funkcji kosztu).\n",
    "   - **Lepsze metody inicjalizacji**: np. k-means++ zaimplementowany w sklearn, który wybiera początkowe centroidy w sposób, który zwiększa ich rozproszenie.\n",
    "   - **Metody hierarchiczne**: Początkowo stosowanie większej liczby klastrów, a następnie łączenie podobnych.\n",
    "   - **Anizotropowe metryki**: Stosowanie miar odległości, które lepiej pasują do kształtu danych.\n",
    "   - **Strojenie parametru k**: Używanie metod takich jak metoda elbow method, silhouette coefficient lub kryterium informacyjne (BIC, AIC) do wybrania optymalnej liczby klastrów.\n",
    "   - **Inne algorytmy grupowania**: Dla danych o złożonych kształtach można rozważyć inne metody, np. DBSCAN, Gaussian Mixture Models czy grupowanie spektralne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 2 - parametry wielowymiarowego rozkładu normalnego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy kod służy do rysowania dwuwymiarowego rozkładu normalnego o wektorze średnich $[0,0]$ i macierzy kowariancji równej $[[1, 0], [0, 1]]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(6, 6), dpi=80)\n",
    "x, y = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "points = np.dstack((x, y))\n",
    "normal = multivariate_normal([0, 0], [[1, 0], [0, 1]])\n",
    "densities = normal.pdf(points)\n",
    "plt.contourf(x, y, densities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź jak zmieni się dwuwymiarowy rozkład normalny, jeżeli zmienisz wektor średnich  na np. $[1,0]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from scipy.stats import multivariate_normal\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(12, 5), dpi=80)\n",
    "\n",
    "# Oryginalny rozkład z wektorem średnich [0, 0]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Wektor średnich [0, 0]')\n",
    "x, y = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "points = np.dstack((x, y))\n",
    "normal = multivariate_normal([0, 0], [[1, 0], [0, 1]])\n",
    "densities = normal.pdf(points)\n",
    "plt.contourf(x, y, densities)\n",
    "\n",
    "# Rozkład z wektorem średnich [1, 0]\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Wektor średnich [1, 0]')\n",
    "normal_shifted = multivariate_normal([1, 0], [[1, 0], [0, 1]])\n",
    "densities_shifted = normal_shifted.pdf(points)\n",
    "plt.contourf(x, y, densities_shifted)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Zmiana wektora średnich powoduje przesunięcie rozkładu w przestrzeni\n",
    "# bez zmiany jego kształtu. Jest to translacja rozkładu w kierunku nowego wektora średnich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź jak zmieni się dwuwymiarowy rozkład normalny, jeżeli zmodyfikujesz macierz kowariancji na wielokrotność macierzy jednostkowej. (Warto wrócić do oryginalnego ustawienia wektora średnich, aby wycentrować wykres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from scipy.stats import multivariate_normal\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 5), dpi=80)\n",
    "\n",
    "# Oryginalny rozkład z macierzą kowariancji [[1, 0], [0, 1]]\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Macierz kowariancji [[1, 0], [0, 1]]')\n",
    "x, y = np.mgrid[-5:5:.01, -5:5:.01]\n",
    "points = np.dstack((x, y))\n",
    "normal = multivariate_normal([0, 0], [[1, 0], [0, 1]])\n",
    "densities = normal.pdf(points)\n",
    "plt.contourf(x, y, densities)\n",
    "\n",
    "# Rozkład z macierzą kowariancji [[0.5, 0], [0, 2]] (różne wartości na diagonali)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Macierz kowariancji [[0.5, 0], [0, 2]]')\n",
    "normal_diag1 = multivariate_normal([0, 0], [[0.5, 0], [0, 2]])\n",
    "densities_diag1 = normal_diag1.pdf(points)\n",
    "plt.contourf(x, y, densities_diag1)\n",
    "\n",
    "# Rozkład z macierzą kowariancji [[3, 0], [0, 0.5]] (różne wartości na diagonali)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Macierz kowariancji [[3, 0], [0, 0.5]]')\n",
    "normal_diag2 = multivariate_normal([0, 0], [[3, 0], [0, 0.5]])\n",
    "densities_diag2 = normal_diag2.pdf(points)\n",
    "plt.contourf(x, y, densities_diag2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Zmiana macierzy kowariancji na inną macierz diagonalną powoduje zmianę kształtu\n",
    "# rozkładu. Gdy wartości na diagonali są różne, rozkład ma kształt elipsoidy,\n",
    "# której osie są równoległe do osi układu współrzędnych.\n",
    "# Większa wartość na pierwszej pozycji diagonali oznacza większe rozproszenie\n",
    "# w kierunku osi X, a większa wartość na drugiej pozycji - w kierunku osi Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź jak zmieni się dwuwymiarowy rozkład normalny, jeżeli zmodyfikujesz macierz kowariancji na inną macierz diagonalną"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from scipy.stats import multivariate_normal\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 5), dpi=80)\n",
    "\n",
    "# Oryginalny rozkład z macierzą kowariancji [[1, 0], [0, 1]]\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Macierz kowariancji [[1, 0], [0, 1]]')\n",
    "x, y = np.mgrid[-5:5:.01, -5:5:.01]\n",
    "points = np.dstack((x, y))\n",
    "normal = multivariate_normal([0, 0], [[1, 0], [0, 1]])\n",
    "densities = normal.pdf(points)\n",
    "plt.contourf(x, y, densities)\n",
    "\n",
    "# Rozkład z macierzą kowariancji [[0.5, 0], [0, 2]] (różne wartości na diagonali)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Macierz kowariancji [[0.5, 0], [0, 2]]')\n",
    "normal_diag1 = multivariate_normal([0, 0], [[0.5, 0], [0, 2]])\n",
    "densities_diag1 = normal_diag1.pdf(points)\n",
    "plt.contourf(x, y, densities_diag1)\n",
    "\n",
    "# Rozkład z macierzą kowariancji [[3, 0], [0, 0.5]] (różne wartości na diagonali)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Macierz kowariancji [[3, 0], [0, 0.5]]')\n",
    "normal_diag2 = multivariate_normal([0, 0], [[3, 0], [0, 0.5]])\n",
    "densities_diag2 = normal_diag2.pdf(points)\n",
    "plt.contourf(x, y, densities_diag2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Zmiana macierzy kowariancji na inną macierz diagonalną powoduje zmianę kształtu\n",
    "# rozkładu. Gdy wartości na diagonali są różne, rozkład ma kształt elipsoidy,\n",
    "# której osie są równoległe do osi układu współrzędnych.\n",
    "# Większa wartość na pierwszej pozycji diagonali oznacza większe rozproszenie\n",
    "# w kierunku osi X, a większa wartość na drugiej pozycji - w kierunku osi Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sprawdź jak zmieni się dwuwymiarowy rozkład normalny, jeżeli zmodyfikujesz macierz kowariancji na inną macierz symetryczną np. $[[1, 0.9], [0.9, 1]]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from scipy.stats import multivariate_normal\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 5), dpi=80)\n",
    "\n",
    "# Oryginalny rozkład z macierzą kowariancji [[1, 0], [0, 1]]\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Macierz kowariancji [[1, 0], [0, 1]]')\n",
    "x, y = np.mgrid[-5:5:.01, -5:5:.01]\n",
    "points = np.dstack((x, y))\n",
    "normal = multivariate_normal([0, 0], [[1, 0], [0, 1]])\n",
    "densities = normal.pdf(points)\n",
    "plt.contourf(x, y, densities)\n",
    "\n",
    "# Rozkład z macierzą kowariancji [[1, 0.5], [0.5, 1]] (umiarkowana korelacja)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Macierz kowariancji [[1, 0.5], [0.5, 1]]')\n",
    "normal_corr1 = multivariate_normal([0, 0], [[1, 0.5], [0.5, 1]])\n",
    "densities_corr1 = normal_corr1.pdf(points)\n",
    "plt.contourf(x, y, densities_corr1)\n",
    "\n",
    "# Rozkład z macierzą kowariancji [[1, 0.9], [0.9, 1]] (silna korelacja)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Macierz kowariancji [[1, 0.9], [0.9, 1]]')\n",
    "normal_corr2 = multivariate_normal([0, 0], [[1, 0.9], [0.9, 1]])\n",
    "densities_corr2 = normal_corr2.pdf(points)\n",
    "plt.contourf(x, y, densities_corr2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Wprowadzenie niezerowych wartości poza diagonalą powoduje, że główne osie\n",
    "# elipsoidy rozkładu nie są już równoległe do osi układu współrzędnych.\n",
    "# Wartości te reprezentują korelację między zmiennymi - im wyższe wartości \n",
    "# (bliższe 1 dla dodatniej korelacji lub -1 dla ujemnej korelacji),\n",
    "# tym silniejsza korelacja i tym bardziej rozkład jest \"obrócony\" względem\n",
    "# osi układu współrzędnych. Elipsoida jest wówczas pochylona wzdłuż linii \n",
    "# dodatniego (lub ujemnego) nachylenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0791f3334a882ed4f32dcac449397d55",
     "grade": false,
     "grade_id": "cell-016724d664d5480d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Macierz kowariancji musi być symetryczna, jednak sama własność symetryczności macierzy nie wystarcza. Spróbuj zwizualizować rozkład o macierzy $[[1, 1], [1, 1]]$. Dlaczego nie jest to możliwe? Wyjaśnij używając zarówno formalizmu matematycznego jak i intuicji. (Jak wyglądałby wykres takiego rozkładu?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Próba utworzenia rozkładu z osobliwą macierzą kowariancji [[1, 1], [1, 1]]\n",
    "try:\n",
    "    singular_normal = multivariate_normal([0, 0], [[1, 1], [1, 1]])\n",
    "    x, y = np.mgrid[-3:3:.01, -3:3:.01]\n",
    "    points = np.dstack((x, y))\n",
    "    densities = singular_normal.pdf(points)\n",
    "    plt.contourf(x, y, densities)\n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(\"Wystąpił błąd:\", e)\n",
    "\n",
    "# Wyjaśnienie matematyczne:\n",
    "# Macierz kowariancji musi być dodatnio określona (lub co najmniej półokreślona dodatnio),\n",
    "# co oznacza, że wszystkie jej wartości własne muszą być dodatnie (lub nieujemne).\n",
    "# \n",
    "# Dla macierzy [[1, 1], [1, 1]] wartości własne to:\n",
    "# - λ₁ = 2 (dodatnia)\n",
    "# - λ₂ = 0 (zerowa)\n",
    "#\n",
    "# Zerowa wartość własna oznacza, że macierz jest osobliwa (nie jest odwracalna),\n",
    "# a więc jej wyznacznik wynosi 0: det([[1, 1], [1, 1]]) = 1*1 - 1*1 = 0.\n",
    "#\n",
    "# W kontekście wielowymiarowego rozkładu normalnego, funkcja gęstości prawdopodobieństwa\n",
    "# zawiera odwrotność macierzy kowariancji oraz jej wyznacznik w mianowniku:\n",
    "# f(x) = (1/((2π)^(k/2) * |Σ|^(1/2))) * exp(-0.5 * (x-μ)^T * Σ^(-1) * (x-μ))\n",
    "#\n",
    "# Gdy macierz jest osobliwa, nie można obliczyć jej odwrotności ani pierwiastka z wyznacznika.\n",
    "#\n",
    "# Interpretacja geometryczna:\n",
    "# Macierz [[1, 1], [1, 1]] reprezentuje idealną korelację między zmiennymi (r=1),\n",
    "# co oznacza, że wszystkie punkty rozkładu leżą dokładnie na linii prostej y = x.\n",
    "# Rozkład ma zerową wariancję w kierunku prostopadłym do tej linii.\n",
    "# Geometrycznie, elipsa rozkładu normalnego zdegenerowała się do odcinka linii prostej,\n",
    "# co nie jest prawidłowym dwuwymiarowym rozkładem prawdopodobieństwa.\n",
    "# Taka dystrybuanta nie całkowałaby się do 1 w przestrzeni dwuwymiarowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korzystając z funkcji `multivariate_normal`, zwizualizuj mieszaninę dwóch rozkładów normalnych:\n",
    "$$P(x) = \\tau N(x; \\mu_1, \\Sigma_1) + (1-\\tau) N(x; \\mu_2, \\Sigma_2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6b076c05355955c600de4f3e7c526f1",
     "grade": false,
     "grade_id": "cell-b3ea989b13e12d41",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from scipy.stats import multivariate_normal\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funkcja do obliczenia gęstości mieszaniny rozkładów\n",
    "def mixture_pdf(points, tau, mu1, sigma1, mu2, sigma2):\n",
    "    dist1 = multivariate_normal(mu1, sigma1)\n",
    "    dist2 = multivariate_normal(mu2, sigma2)\n",
    "    return tau * dist1.pdf(points) + (1 - tau) * dist2.pdf(points)\n",
    "\n",
    "# Siatka punktów\n",
    "x, y = np.mgrid[-5:5:.01, -5:5:.01]\n",
    "points = np.dstack((x, y))\n",
    "\n",
    "# Parametry mieszaniny\n",
    "tau = 0.7  # waga pierwszego rozkładu\n",
    "mu1 = [-2, -1]  # średnia pierwszego rozkładu\n",
    "sigma1 = [[1, 0], [0, 1]]  # kowariancja pierwszego rozkładu\n",
    "mu2 = [2, 2]  # średnia drugiego rozkładu\n",
    "sigma2 = [[1, 0.5], [0.5, 1]]  # kowariancja drugiego rozkładu\n",
    "\n",
    "# Obliczenie gęstości dla mieszaniny\n",
    "mixture_densities = mixture_pdf(points, tau, mu1, sigma1, mu2, sigma2)\n",
    "\n",
    "# Wizualizacja\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Pierwszy rozkład\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(f'Pierwszy rozkład (waga: {tau:.1f})')\n",
    "dist1 = multivariate_normal(mu1, sigma1)\n",
    "plt.contourf(x, y, dist1.pdf(points))\n",
    "\n",
    "# Drugi rozkład\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(f'Drugi rozkład (waga: {1-tau:.1f})')\n",
    "dist2 = multivariate_normal(mu2, sigma2)\n",
    "plt.contourf(x, y, dist2.pdf(points))\n",
    "\n",
    "# Mieszanina\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Mieszanina rozkładów')\n",
    "plt.contourf(x, y, mixture_densities)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Wizualizacja mieszanin z różnymi wartościami tau\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, t in enumerate([0.1, 0.3, 0.5, 0.7, 0.9]):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.title(f'tau = {t:.1f}')\n",
    "    mix = mixture_pdf(points, t, mu1, sigma1, mu2, sigma2)\n",
    "    plt.contourf(x, y, mix)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ćwiczenia**\n",
    "1. Poeksperymentuj z różnymi ustawieniami mieszaniny. Na co wpływa parametr $\\tau$?\n",
    "2. Rozważając mieszaninę rozkładów normalnych o dowolnej liczbie komponentów - czy istnieją rozkłady, których nie można zamodelować? Odpowiedź uzasadnij używając zdobytych intuicji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Odpowiedzi na pytania:**\n",
    "\n",
    "1. **Na co wpływa parametr τ?**\n",
    "   \n",
    "   Parametr τ (tau) to waga pierwszego komponentu w mieszaninie rozkładów, czyli proporcja, z jaką pierwszy rozkład normalny przyczynia się do całkowitej mieszaniny. Odpowiednio (1-τ) to waga drugiego komponentu.\n",
    "   \n",
    "   - Kiedy τ = 1, mieszanina staje się po prostu pierwszym rozkładem normalnym.\n",
    "   - Kiedy τ = 0, mieszanina staje się drugim rozkładem normalnym.\n",
    "   - Dla wartości pośrednich, τ określa względny \"udział\" każdego z komponentów w ostatecznym rozkładzie.\n",
    "   \n",
    "   Na wizualizacjach widać, jak dla różnych wartości τ zmienia się kształt mieszaniny - w miejscach, gdzie dominuje komponent o większej wadze, gęstość prawdopodobieństwa jest większa.\n",
    "   \n",
    "2. **Czy istnieją rozkłady, których nie można zamodelować mieszaniną rozkładów normalnych?**\n",
    "   \n",
    "   Teoretycznie, przy odpowiednio dużej liczbie komponentów mieszaniny, można aproksymować dowolny ciągły rozkład prawdopodobieństwa z dowolną dokładnością. Jest to konsekwencją twierdzenia o uniwersalnej aproksymacji. Jednak w praktyce istnieją pewne ograniczenia:\n",
    "   \n",
    "   - **Rozkłady z nieciągłościami**: Mieszanina rozkładów normalnych jest zawsze ciągła, więc nie może idealnie odwzorować rozkładów z punktami nieciągłości.\n",
    "   \n",
    "   - **Rozkłady o ciężkich ogonach**: Rozkłady takie jak rozkład Cauchy'ego, które mają bardzo \"ciężkie ogony\" (gdzie prawdopodobieństwo wystąpienia wartości odległych od średniej jest znacznie większe niż w rozkładzie normalnym), są trudne do dokładnego zamodelowania za pomocą skończonej liczby komponentów normalnych.\n",
    "   \n",
    "   - **Rozkłady z ostrymi szczytami**: Rozkłady z bardzo ostrymi szczytami (jak rozkład Laplace'a) wymagają dużej liczby komponentów do dokładnej aproksymacji.\n",
    "   \n",
    "   - **Rozkłady o złożonej strukturze przestrzennej**: Rozkłady z wieloma lokalnymi maksimami, wzorami fraktalnymi lub osobliwościami wymagałyby bardzo dużej liczby komponentów.\n",
    "   \n",
    "   W praktyce jednak mieszaniny rozkładów normalnych są niezwykle elastyczne i mogą aproksymować większość rozkładów występujących w rzeczywistych danych z wystarczającą dokładnością, jeśli użyje się odpowiedniej liczby komponentów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadanie 3 - EM dla mieszaniny jednowymiarowej\n",
    "Dane są dane wygenerowane z poniższej mieszaniny rozkładów normalnych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "x_axis = np.arange(-10, 10, 0.001)\n",
    "plt.plot(x_axis, 0.66 * norm.pdf(x_axis, 0, 2) + 0.33 * norm.pdf(x_axis, 5, 2))\n",
    "plt.show()\n",
    "\n",
    "data = np.concatenate((np.random.normal(0, 2, 66), np.random.normal(5, 2, 33)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwizualizuj dane na histogramie. Zwróć uwagę, że dobór odpowiednich $\\mu_1, \\mu_2, \\sigma_1, \\sigma_2$ na podstawie danych nie jest oczywisty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj algorytm EM dla powyższego problemu. Zwizualizuj kolejne kroki algorytmu (tj. narysuj wykres mikstury co kilka iteracji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8614fb4fde6c6108d0b6edf49b478d75",
     "grade": true,
     "grade_id": "cell-c932922d874bfaf2",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Funkcja do wizualizacji mieszaniny\n",
    "def plot_mixture(data, pi, mu, sigma, iteration=None):\n",
    "    x_grid = np.linspace(np.min(data) - 2, np.max(data) + 2, 1000)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Histogram danych\n",
    "    plt.hist(data, bins=30, density=True, alpha=0.5, color='gray', label='Dane')\n",
    "    \n",
    "    # Komponenty mieszaniny\n",
    "    plt.plot(x_grid, pi[0] * norm.pdf(x_grid, mu[0], sigma[0]), 'r--', \n",
    "             linewidth=2, label=f'Komponent 1: π={pi[0]:.2f}, μ={mu[0]:.2f}, σ={sigma[0]:.2f}')\n",
    "    plt.plot(x_grid, pi[1] * norm.pdf(x_grid, mu[1], sigma[1]), 'b--', \n",
    "             linewidth=2, label=f'Komponent 2: π={pi[1]:.2f}, μ={mu[1]:.2f}, σ={sigma[1]:.2f}')\n",
    "    \n",
    "    # Całkowita mieszanina\n",
    "    mixture = pi[0] * norm.pdf(x_grid, mu[0], sigma[0]) + pi[1] * norm.pdf(x_grid, mu[1], sigma[1])\n",
    "    plt.plot(x_grid, mixture, 'g-', linewidth=3, label='Mieszanina')\n",
    "    \n",
    "    if iteration is not None:\n",
    "        plt.title(f'Mieszanina rozkładów normalnych - iteracja {iteration}')\n",
    "    else:\n",
    "        plt.title('Mieszanina rozkładów normalnych')\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Gęstość prawdopodobieństwa')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Implementacja algorytmu EM dla mieszaniny dwóch rozkładów normalnych\n",
    "def em_gaussian_mixture(data, max_iter=100, tol=1e-6, verbose=True):\n",
    "    n_samples = len(data)\n",
    "    \n",
    "    # Inicjalizacja parametrów\n",
    "    # Prosta heurestyka do inicjalizacji\n",
    "    # Wybieramy dwa różne punkty jako początkowe średnie\n",
    "    pi = np.array([0.5, 0.5])  # Wagi komponentów\n",
    "    mu = np.array([np.mean(data) - np.std(data), np.mean(data) + np.std(data)])  # Średnie\n",
    "    sigma = np.array([np.std(data), np.std(data)])  # Odchylenia standardowe\n",
    "    \n",
    "    log_likelihood_old = -np.inf\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Odpowiedzialności\n",
    "        resp = np.zeros((n_samples, 2))\n",
    "        \n",
    "        for k in range(2):\n",
    "            resp[:, k] = pi[k] * norm.pdf(data, mu[k], sigma[k])\n",
    "        \n",
    "        # Normalizacja odpowiedzialności\n",
    "        resp_sum = resp.sum(axis=1, keepdims=True)\n",
    "        resp = resp / resp_sum\n",
    "        \n",
    "        # Aktualizacja parametrów\n",
    "        N_k = resp.sum(axis=0)\n",
    "        \n",
    "        # Aktualizacja wag\n",
    "        pi = N_k / n_samples\n",
    "        \n",
    "        # Aktualizacja średnich\n",
    "        mu = np.array([np.sum(resp[:, k] * data) / N_k[k] for k in range(2)])\n",
    "        \n",
    "        # Aktualizacja wariancji\n",
    "        sigma = np.array([np.sqrt(np.sum(resp[:, k] * (data - mu[k])**2) / N_k[k]) for k in range(2)])\n",
    "        \n",
    "        # Obliczenie log-wiarygodności\n",
    "        log_likelihood = np.sum(np.log(pi[0] * norm.pdf(data, mu[0], sigma[0]) + \n",
    "                                       pi[1] * norm.pdf(data, mu[1], sigma[1])))\n",
    "        \n",
    "        # Wizualizacja co 5 iteracji\n",
    "        if verbose and (iteration % 5 == 0 or iteration == max_iter - 1):\n",
    "            plot_mixture(data, pi, mu, sigma, iteration)\n",
    "        \n",
    "        # Sprawdzenie zbieżności\n",
    "        if np.abs(log_likelihood - log_likelihood_old) < tol:\n",
    "            if verbose:\n",
    "                print(f\"Zbieżność osiągnięta po {iteration + 1} iteracjach\")\n",
    "            break\n",
    "        \n",
    "        log_likelihood_old = log_likelihood\n",
    "        \n",
    "    return pi, mu, sigma, log_likelihood\n",
    "\n",
    "# Uruchomienie algorytmu EM na danych\n",
    "pi_est, mu_est, sigma_est, log_likelihood = em_gaussian_mixture(data, max_iter=30)\n",
    "\n",
    "print(\"Parametry modelu po zbieżności:\")\n",
    "print(f\"π = {pi_est}\")\n",
    "print(f\"μ = {mu_est}\")\n",
    "print(f\"σ = {sigma_est}\")\n",
    "print(f\"Log-wiarygodność = {log_likelihood}\")\n",
    "\n",
    "# Porównanie z oryginalnymi parametrami\n",
    "print(\"\\nOryginalne parametry:\")\n",
    "print(\"π = [0.66, 0.33]\")\n",
    "print(\"μ = [0, 5]\")\n",
    "print(\"σ = [2, 2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skorzystaj z gotowej implementacji EM-GMM w pakiecie `sklearn` dla danych z elipsami:\n",
    "```\n",
    "from helpers import get_quasispherical_data\n",
    "X = get_quasispherical_data()\n",
    "```\n",
    "\n",
    "Czy algorytm EM lepiej poradził sobie z tymi danymi niż k-Means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from helpers import get_quasispherical_data\n",
    "\n",
    "# Wczytanie danych quasispherical\n",
    "X_quasi = get_quasispherical_data()\n",
    "\n",
    "# Wizualizacja danych\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Dane quasispherical\")\n",
    "plt.scatter(X_quasi[:, 0], X_quasi[:, 1], s=50)\n",
    "plt.show()\n",
    "\n",
    "# Zastosowanie algorytmu EM-GMM z sklearn\n",
    "gmm = GaussianMixture(n_components=3, random_state=0)\n",
    "gmm.fit(X_quasi)\n",
    "labels_gmm = gmm.predict(X_quasi)\n",
    "\n",
    "# Wizualizacja wyników grupowania EM-GMM\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title(\"Grupowanie EM-GMM (k=3)\")\n",
    "plt.scatter(X_quasi[:, 0], X_quasi[:, 1], c=labels_gmm, s=50, cmap='viridis')\n",
    "\n",
    "# Dla porównania - grupowanie k-means\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=0)\n",
    "kmeans.fit(X_quasi)\n",
    "labels_kmeans = kmeans.labels_\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title(\"Grupowanie k-means (k=3)\")\n",
    "plt.scatter(X_quasi[:, 0], X_quasi[:, 1], c=labels_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "# Wizualizacja elips kowariancji dla EM-GMM\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title(\"EM-GMM z elipsami kowariancji\")\n",
    "\n",
    "# Funkcja do rysowania elips\n",
    "def draw_ellipse(position, covariance, ax=None, **kwargs):\n",
    "    \"\"\"Rysowanie elipsy odpowiadającej zadanej macierzy kowariancji\"\"\"\n",
    "    from matplotlib.patches import Ellipse\n",
    "    import numpy as np\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    # Obliczenie wartości i wektorów własnych macierzy kowariancji\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    \n",
    "    # Narysowanie elipsy\n",
    "    for nsig in range(1, 4):\n",
    "        ax.add_patch(Ellipse(position, nsig * width, nsig * height,\n",
    "                            angle=angle, **kwargs))\n",
    "\n",
    "# Rysowanie punktów danych z kolorami przypisanymi przez GMM\n",
    "plt.scatter(X_quasi[:, 0], X_quasi[:, 1], c=labels_gmm, s=50, cmap='viridis')\n",
    "\n",
    "# Rysowanie elips kowariancji dla każdego komponentu GMM\n",
    "for i, (mean, cov) in enumerate(zip(gmm.means_, gmm.covariances_)):\n",
    "    draw_ellipse(mean, cov, alpha=0.2, color=f'C{i}')\n",
    "    plt.scatter(mean[0], mean[1], s=100, marker='*', edgecolors='k', color=f'C{i}')\n",
    "\n",
    "# Porównanie log-wiarygodności obu modeli\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title(\"Porównanie modeli\")\n",
    "plt.text(0.1, 0.6, f\"EM-GMM log-likelihood: {gmm.score(X_quasi):.2f}\", fontsize=12)\n",
    "plt.text(0.1, 0.4, f\"K-means inertia: {-kmeans.inertia_:.2f}\", fontsize=12)\n",
    "plt.text(0.1, 0.2, \"GMM lepiej modeluje eliptyczne kształty\\nze względu na macierze kowariancji\", fontsize=12)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Czy EM-GMM lepiej radzi sobie z danymi quasispherical niż k-means?\n",
    "# Tak, EM-GMM znacznie lepiej radzi sobie z takimi danymi. GMM może modelować eliptyczne skupienia\n",
    "# o dowolnej orientacji w przestrzeni, ponieważ każdy komponent ma własną macierz kowariancji.\n",
    "# W przeciwieństwie do k-means, który zakłada jedynie sferyczne skupienia oparte na odległości euklidesowej,\n",
    "# GMM jest w stanie dopasować się do anizotropowej natury danych poprzez dostosowanie kierunku,\n",
    "# orientacji i rozmiaru elipsoidy dla każdego komponentu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykonaj proces strojenia parametru $K$ metody EM-GMM poprzez narysowanie wykresu funkcji wiarygodności dla różnych $K$ na zbiorze uczącym i testowym. Sprawdź czy wybrana liczba elementów mikstury odpowiada intuicji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "104093a7e8ed8cf35b17f46ccc2d9a34",
     "grade": true,
     "grade_id": "cell-704268ce63f247ba",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Wczytanie danych quasispherical\n",
    "from helpers import get_quasispherical_data\n",
    "X_quasi = get_quasispherical_data()\n",
    "\n",
    "# Podział danych na zbiór treningowy i testowy\n",
    "X_train, X_test = train_test_split(X_quasi, test_size=0.3, random_state=42)\n",
    "\n",
    "# Sprawdzenie różnych wartości k\n",
    "k_values = range(1, 11)\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    gmm = GaussianMixture(n_components=k, random_state=42)\n",
    "    gmm.fit(X_train)\n",
    "    \n",
    "    # Obliczenie log-wiarygodności na zbiorze treningowym i testowym\n",
    "    train_scores.append(gmm.score(X_train))\n",
    "    test_scores.append(gmm.score(X_test))\n",
    "    \n",
    "    # Obliczenie BIC i AIC\n",
    "    bic_scores.append(gmm.bic(X_train))\n",
    "    aic_scores.append(gmm.aic(X_train))\n",
    "\n",
    "# Wizualizacja wyników\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Wykres log-wiarygodności\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(k_values, train_scores, 'o-', label='Zbiór treningowy')\n",
    "plt.plot(k_values, test_scores, 'x-', label='Zbiór testowy')\n",
    "plt.xlabel('Liczba komponentów (k)')\n",
    "plt.ylabel('Log-wiarygodność')\n",
    "plt.title('Log-wiarygodność dla różnych wartości k')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Wykres BIC\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(k_values, bic_scores, 'o-')\n",
    "plt.xlabel('Liczba komponentów (k)')\n",
    "plt.ylabel('BIC')\n",
    "plt.title('Kryterium informacyjne Bayesa (BIC)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Wykres AIC\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(k_values, aic_scores, 'o-')\n",
    "plt.xlabel('Liczba komponentów (k)')\n",
    "plt.ylabel('AIC')\n",
    "plt.title('Kryterium informacyjne Akaike (AIC)')\n",
    "plt.grid(True)\n",
    "\n",
    "# Znalezienie optymalnej liczby komponentów na podstawie BIC\n",
    "optimal_k_bic = k_values[np.argmin(bic_scores)]\n",
    "optimal_k_aic = k_values[np.argmin(aic_scores)]\n",
    "\n",
    "# Podsumowanie\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.axis('off')\n",
    "plt.text(0.1, 0.9, f\"Optymalna liczba komponentów według BIC: {optimal_k_bic}\", fontsize=12)\n",
    "plt.text(0.1, 0.8, f\"Optymalna liczba komponentów według AIC: {optimal_k_aic}\", fontsize=12)\n",
    "plt.text(0.1, 0.6, \"Intuicyjna liczba komponentów dla danych quasispherical: 3\", fontsize=12)\n",
    "plt.text(0.1, 0.4, \"AIC ma tendencję do wybierania bardziej złożonych modeli,\\npodczas gdy BIC silniej je karze.\", fontsize=12)\n",
    "plt.text(0.1, 0.2, \"BIC jest bardziej konserwatywne i często bardziej odpowiednie\\ndo estymacji prawdziwej liczby skupisk.\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Wizualizacja grupowania dla optymalnej wartości k\n",
    "optimal_k = optimal_k_bic  # Wybieramy wartość z BIC, która jest bardziej konserwatywna\n",
    "gmm_optimal = GaussianMixture(n_components=optimal_k, random_state=42)\n",
    "gmm_optimal.fit(X_quasi)\n",
    "labels_optimal = gmm_optimal.predict(X_quasi)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(f\"Grupowanie EM-GMM z optymalną liczbą komponentów k={optimal_k}\")\n",
    "plt.scatter(X_quasi[:, 0], X_quasi[:, 1], c=labels_optimal, s=50, cmap='viridis')\n",
    "\n",
    "# Rysowanie elips kowariancji\n",
    "for i, (mean, cov) in enumerate(zip(gmm_optimal.means_, gmm_optimal.covariances_)):\n",
    "    draw_ellipse(mean, cov, alpha=0.2, color=f'C{i}')\n",
    "    plt.scatter(mean[0], mean[1], s=100, marker='*', edgecolors='k', color=f'C{i}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optymalna liczba komponentów zgodnie z BIC to {optimal_k_bic}.\")\n",
    "print(f\"Optymalna liczba komponentów zgodnie z AIC to {optimal_k_aic}.\")\n",
    "\n",
    "# Na podstawie wizualnej inspekcji danych, intuicyjnie oczekiwalibyśmy 3 komponentów.\n",
    "# BIC często daje bardziej wiarygodną ocenę prawdziwej liczby komponentów,\n",
    "# ponieważ silniej karze za zbyt złożone modele w porównaniu do AIC."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
